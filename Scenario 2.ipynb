{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2 - ESMValCore\n",
    "\n",
    "This notebook proposes a design for the ESMValTool/ESMValCore API. The code in this notebook has been heavily mocked to demonstrate the potential usage.\n",
    "\n",
    "In this scenario, we look at the API from a recipe developers perspective, a climate scientist who is developing his/her own code to perform some calculations and wants to implement those using ESMValTool so that it can be easily shared with others.\n",
    "\n",
    "For this example, we want to create the climwip diagnostic from scratch, define the datasets and variables, apply preprocessor functions, and save the workflow as a recipe. \n",
    "\n",
    "**Steps**\n",
    "- Reconstruct ClimWIP recipe from scratch using API calls like (see ESMValCore [#489](https://github.com/ESMValGroup/ESMValCore/issues/498)):\n",
    "- Define datasets/datasetlists\n",
    "- Apply preprocessor functions\n",
    "- Call diagnostic script\n",
    "- Save workflow as recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the api and initialize an empty recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esmvaltool_mockup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "The datasets in ESMValTool are split into two parts. The shared part (usually defined by the `datasets` keyword in the recipe) and the variable group (usually defined as part of the `variables` section in the diagnostics). While this makes sense in the context of keeping the recipes short, we have more flexibility in Python to define this using dictionaries. The combination of the variable / dataset is refered to as a `DatasetReference` here. It refers to a dataset, but is only realized later on.\n",
    "\n",
    "We want to define two lists of datasets, one for `tas`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tas_1 = DatasetReference(\n",
    "    dataset='ACCESS1-0',\n",
    "    project='CMIP5',\n",
    "    exp=['historical', 'rcp45'],\n",
    "    ensemble='r1i1p1',\n",
    "    short_name='tas',\n",
    "    start_year=1995,\n",
    "    end_year=2014,\n",
    "    mip='Amon'\n",
    ")\n",
    "\n",
    "dataset_tas_2 = DatasetReference(\n",
    "    dataset='ACCESS1-0',\n",
    "    project='CMIP5',\n",
    "    exp=['historical', 'rcp85'],\n",
    "    ensemble='r1i1p1',\n",
    "    short_name='tas',\n",
    "    start_year=1995,\n",
    "    end_year=2014,\n",
    "    mip='Amon'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset decriptions can be used to locate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/data/CMIP5/tas_ACCESS1-0_CMIP5_Amon_historical-rcp45_r1i1p1_1995-2014.nc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tas_1.locate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, the locations can be used to check if the data are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tas_1.is_available()  # True/False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the data, add them to a `DatasetList`. This object knows how to compute and work with the data, and can be used to apply preprocessors to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetList('NewDatasetList',\n",
       "queue=(PreprocessorStep('load_data'),\n",
       " PreprocessorStep('cmor_check')),\n",
       "datasets=(DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp45'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995}), DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp85'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995})))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_tas = DatasetList(dataset_tas_1, dataset_tas_2)\n",
    "datasets_tas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to stop here, you can get the data from the `DatasetList` directly. This will run the required fixes / checks and return the data as iris cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PreprocessorStep('load_data')\n",
      "Running PreprocessorStep('cmor_check')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cubes'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_tas.get_cubes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for `pr`... This being Python, we can isolate the common parameters into a seperate dictionary, and use dictionary expansion to add the keys to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetList('NewDatasetList',\n",
       "queue=(PreprocessorStep('load_data'),\n",
       " PreprocessorStep('cmor_check')),\n",
       "datasets=(DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp45'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995}), DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp85'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995})))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_keys = {\n",
    " 'dataset': 'ACCESS1-0',\n",
    " 'project': 'CMIP5',\n",
    " 'ensemble': 'r1i1p1',\n",
    " 'short_name': 'tas',\n",
    " 'start_year': 1995,\n",
    " 'end_year': 2014,\n",
    " 'mip': 'Amon'\n",
    "}\n",
    "\n",
    "dataset_pr_1 = DatasetReference(exp=['historical', 'rcp45'], **common_keys)\n",
    "dataset_pr_2 = DatasetReference(exp=['historical', 'rcp85'], **common_keys)\n",
    "\n",
    "datasets_pr = DatasetList(dataset_pr_1, dataset_pr_2)\n",
    "datasets_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors\n",
    "\n",
    "There are several ways to declare the preprocessors. The most basic way most closely represents the recipe `.yml` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocessor('climwip_general')\n",
       "- regrid\n",
       "- mask_landsea\n",
       "- extract_region"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = {\n",
    "    'regrid': {\n",
    "      'target_grid': '2.5x2.5',\n",
    "      'scheme': 'linear',\n",
    "    },\n",
    "    'mask_landsea': {\n",
    "      'mask_out': 'sea',\n",
    "    },\n",
    "    'extract_region': {\n",
    "      'start_longitude': -10.0,\n",
    "      'end_longitude': 39.0,\n",
    "      'start_latitude': 30.0,\n",
    "      'end_latitude': 76.25,\n",
    "    }\n",
    "}\n",
    "\n",
    "climwip_general = Preprocessor(name='climwip_general', steps=steps)\n",
    "climwip_general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is possible to use a custom order using `custom_order=True`, ESMValTool tries to determine the scientifically most sensible order from the steps given. This order can be retrieved using the `.order` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regrid', 'mask_landsea', 'extract_region']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climwip_general.order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect each individual `PreprocessorStep` and its settings, note that `.steps` is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessorStep('regrid', target_grid=2.5x2.5, scheme=linear)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climwip_general.steps['regrid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the steps have been defined once for `climwip_general`, we can use those as a basis for the `climatological_mean` preprocessor, which adds a `climate_statistics` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regrid': PreprocessorStep('regrid', target_grid=2.5x2.5, scheme=linear),\n",
       " 'mask_landsea': PreprocessorStep('mask_landsea', mask_out=sea),\n",
       " 'extract_region': PreprocessorStep('extract_region', start_longitude=-10.0, end_longitude=39.0, start_latitude=30.0, end_latitude=76.25),\n",
       " 'climate_statistics': PreprocessorStep('climate_statistics', operator=mean)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_statistics_mean = PreprocessorStep(function='climate_statistics', operator='mean')\n",
    "\n",
    "steps = climwip_general.steps.copy()  # make a copy\n",
    "steps['climate_statistics'] = climate_statistics_mean\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the steps have been defined, a new `Preprocessor` can be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatological_mean = Preprocessor(name='climatological_mean', steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more straightforward is to consider `climwip_general` as a Preprocessor step by itself, so a shortcut for the code above would be to pass it as a step by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrend = PreprocessorStep(\n",
    "    function='detrend', \n",
    "    dimension='time', \n",
    "    method='linear'\n",
    ")\n",
    "\n",
    "climate_statistics_std = PreprocessorStep(\n",
    "    function='climate_statistics', \n",
    "    operator='std_dev'\n",
    ")\n",
    "\n",
    "detrended_std = Preprocessor(\n",
    "    name='detrended_std', \n",
    "    steps=[climwip_general, detrend, climate_statistics_std]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we may be lazy and already have some yaml code lying around from another recipe that we want to use. For this, we can use the `.from_yaml` classmethod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocessor('temperature_anomalies')\n",
       "- area_statistics\n",
       "- annual_statistics\n",
       "- anomalies"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_code = \"\"\"\n",
    "  temperature_anomalies:\n",
    "    area_statistics:\n",
    "      operator: mean\n",
    "    annual_statistics:\n",
    "      operator: mean\n",
    "    anomalies:\n",
    "      period: full\n",
    "      reference:\n",
    "        start_year: 1981\n",
    "        start_month: 1\n",
    "        start_day: 1\n",
    "        end_year: 2010\n",
    "        end_month: 12\n",
    "        end_day: 31\n",
    "      standardize: false\n",
    "\"\"\"\n",
    "      \n",
    "preproc = Preprocessor.from_yaml(yaml_code)\n",
    "preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a preprocessor\n",
    "\n",
    "`DatasetLists` are used to play with variables and preprocessor. Preprocessors/steps can be applied directly to a `DatasetList`. This can happen in place, or when a new instance of the object is returned (with `in_place=False`). This allows datasets to be forked re-used. The steps are tracked in a *steps queue*.\n",
    "\n",
    "Here, the `DatasetList` is inialized with the the `climwip_general` preprocessor. The steps are applied when `.compute` is called. Note that the `queue` is then empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PreprocessorStep('load_data')\n",
      "Running PreprocessorStep('cmor_check')\n",
      "Running Preprocessor('climwip_general')\n",
      " -> PreprocessorStep('regrid')\n",
      " -> PreprocessorStep('mask_landsea')\n",
      " -> PreprocessorStep('extract_region')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetList('my first dataset',\n",
       "queue=(),\n",
       "datasets=(DatasetList('NewDatasetList',\n",
       "queue=(PreprocessorStep('load_data'),\n",
       " PreprocessorStep('cmor_check')),\n",
       "datasets=(DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp45'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995}), DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp85'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995}))),))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_pr = DatasetList(\n",
    "    datasets_pr,\n",
    "    name='my first dataset',\n",
    "    preprocessor=climwip_general,\n",
    ")\n",
    "\n",
    "ds_pr.compute()\n",
    "ds_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the data can be initialized using the individual datasets and without a preprocessor. In this example, the steps are added one-by-one and new `DatasetList` objects are returned. This gives some more flexibility to fork the items. Note that the queue is still full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetList('NewDatasetList',\n",
       "queue=(PreprocessorStep('load_data'),\n",
       " PreprocessorStep('cmor_check'),\n",
       " PreprocessorStep('regrid'),\n",
       " PreprocessorStep('mask_landsea'),\n",
       " PreprocessorStep('extract_region')),\n",
       "datasets=(DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp45'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995}), DatasetReference({'dataset': 'ACCESS1-0',\n",
       " 'end_year': 2014,\n",
       " 'ensemble': 'r1i1p1',\n",
       " 'exp': ['historical', 'rcp85'],\n",
       " 'mip': 'Amon',\n",
       " 'project': 'CMIP5',\n",
       " 'short_name': 'tas',\n",
       " 'start_year': 1995})))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_pr = DatasetList(dataset_pr_1, dataset_pr_2)\n",
    "\n",
    "ds_regridded = ds_pr.add_step(\n",
    "    function='regrid',\n",
    "    target_grid='2.5x2.5', \n",
    "    scheme='linear',\n",
    "    in_place=False,\n",
    ")  # lazy\n",
    "\n",
    "ds_masked = ds_regridded.add_step(\n",
    "    function='mask_landsea',\n",
    "    mask_out='sea',\n",
    "    in_place=False,\n",
    ")\n",
    "\n",
    "ds_region = ds_masked.add_step(\n",
    "    function='extract_region',\n",
    "    start_longitude= -10.0, \n",
    "    end_longitude= 39.0,\n",
    "    start_latitude= 30.0,\n",
    "    end_latitude= 76.25,\n",
    "    in_place=False,\n",
    ")\n",
    "\n",
    "ds_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queue can be printed using `.print_q`, and shows the steps that have been performed / are to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps queue\n",
      "-----------\n",
      "Todo -> PreprocessorStep('load_data')\n",
      "Todo -> PreprocessorStep('cmor_check')\n",
      "Todo -> PreprocessorStep('regrid')\n",
      "Todo -> PreprocessorStep('mask_landsea')\n",
      "Todo -> PreprocessorStep('extract_region')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_region.print_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation is lazy, and execution is delayed until the data are requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PreprocessorStep('load_data')\n",
      "Running PreprocessorStep('cmor_check')\n",
      "Running PreprocessorStep('regrid')\n",
      "Running PreprocessorStep('mask_landsea')\n",
      "Running PreprocessorStep('extract_region')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cubes'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_region.compute()            # consume steps, optional\n",
    "cubes = ds_region.get_cubes()  # return data cubes\n",
    "cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performed steps are now marked as *Done*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps queue\n",
      "-----------\n",
      "Done -> PreprocessorStep('load_data')\n",
      "Done -> PreprocessorStep('cmor_check')\n",
      "Done -> PreprocessorStep('regrid')\n",
      "Done -> PreprocessorStep('mask_landsea')\n",
      "Done -> PreprocessorStep('extract_region')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_region.print_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the cubes are cached on this item. Asking for the cubes again, will return the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cubes'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_region.get_cubes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that new steps can be easily added, as the intermediate result is stored. For example, to recreate the `climatological_mean` preprocessor, we apply the `climate_statistics_mean` step defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessorStep('climate_statistics', operator=mean)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_statistics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps queue\n",
      "-----------\n",
      "Done -> PreprocessorStep('load_data')\n",
      "Done -> PreprocessorStep('cmor_check')\n",
      "Done -> PreprocessorStep('regrid')\n",
      "Done -> PreprocessorStep('mask_landsea')\n",
      "Done -> PreprocessorStep('extract_region')\n",
      "Todo -> PreprocessorStep('climate_statistics')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "climstats = ds_region.add_step(climate_statistics_mean, in_place=False)\n",
    "climstats.print_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how three steps are already marked as `Done`. Requesting the data only executes the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PreprocessorStep('climate_statistics')\n"
     ]
    }
   ],
   "source": [
    "cubes = climstats.get_cubes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most imporantly, the state of our original dataset list has not been changed, so we can still use that for some other data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps queue\n",
      "-----------\n",
      "Todo -> PreprocessorStep('load_data')\n",
      "Todo -> PreprocessorStep('cmor_check')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_pr.print_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to apply a preprocessor directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps queue\n",
      "-----------\n",
      "Todo -> PreprocessorStep('load_data')\n",
      "Todo -> PreprocessorStep('cmor_check')\n",
      "Todo -> Preprocessor('detrended_std')\n",
      "\n",
      "Running PreprocessorStep('load_data')\n",
      "Running PreprocessorStep('cmor_check')\n",
      "Running Preprocessor('detrended_std')\n",
      " -> PreprocessorStep('regrid')\n",
      " -> PreprocessorStep('mask_landsea')\n",
      " -> PreprocessorStep('extract_region')\n",
      " -> PreprocessorStep('detrend')\n",
      " -> PreprocessorStep('climate_statistics')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cubes'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_detrended = ds_pr.add_step(detrended_std, in_place=False)\n",
    "\n",
    "data_detrended.print_queue()\n",
    "\n",
    "cubes = data_detrended.get_cubes()\n",
    "cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic\n",
    "\n",
    "The diagnostic consists of the name of a script to call, as well as the ancestors to use. Although in Python we can call `.get_cubes` and play with the data directly, for the purpose of this notebook we want to use the data to call a diagnostic.\n",
    "The diagnostic must know on which data to run. These are defined by the ancestors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tas = DatasetList(\n",
    "    dataset_tas_1, dataset_tas_2,\n",
    "    name='dataset_tas',\n",
    "    preprocessor=detrended_std,\n",
    ")\n",
    "\n",
    "ds_pr = DatasetList(\n",
    "    dataset_pr_1, dataset_pr_2,\n",
    "    name='dataset_pr',\n",
    "    preprocessor=climatological_mean,\n",
    ")\n",
    "\n",
    "ancestors = ds_tas, ds_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the name of the script we want to run `weighting/climwip.py`. The remaining parameters are passed to the script directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_params = {\n",
    "  'tas': {\n",
    "    'sigma_d': 0.588,\n",
    "    'sigma_s': 0.704,\n",
    "  },\n",
    "  'pr': {\n",
    "    'sigma_d': 0.658,\n",
    "    'sigma_s': 0.704, \n",
    "  }\n",
    "}\n",
    "\n",
    "calculate_weights_climwip = Diagnostic(\n",
    "    name='climwip',\n",
    "    script='weighting/climwip.py', \n",
    "    ancestors=ancestors, \n",
    "    shape_params=shape_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a diagnostic directly, and pass the ancestors directly to the script. ESMValTool will run the diagnostic in a new session directory, but if we want to change the session, we can pass the session name. ESMValTool will make a new timestamped directory and store the data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session directory: diag_climwip-20201106_153837\n",
      "\n",
      "Working on 'dataset_tas'\n",
      "Running PreprocessorStep('load_data')\n",
      "Running PreprocessorStep('cmor_check')\n",
      "Running Preprocessor('detrended_std')\n",
      " -> PreprocessorStep('regrid')\n",
      " -> PreprocessorStep('mask_landsea')\n",
      " -> PreprocessorStep('extract_region')\n",
      " -> PreprocessorStep('detrend')\n",
      " -> PreprocessorStep('climate_statistics')\n",
      "\n",
      "Working on 'dataset_pr'\n",
      "Running PreprocessorStep('load_data')\n",
      "Running PreprocessorStep('cmor_check')\n",
      "Running Preprocessor('climatological_mean')\n",
      " -> PreprocessorStep('regrid')\n",
      " -> PreprocessorStep('mask_landsea')\n",
      " -> PreprocessorStep('extract_region')\n",
      " -> PreprocessorStep('climate_statistics')\n",
      "\n",
      "Running script 'weighting/climwip.py'\n"
     ]
    }
   ],
   "source": [
    "calculate_weights_climwip.run(session='diag_climwip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the recipe\n",
    "\n",
    "In general, a ESMValTool recipe consists of 4 parts:\n",
    "- documentation\n",
    "- datasets\n",
    "- preprocessors\n",
    "- diagnostics\n",
    "\n",
    "All but the documentation has already been defined. The key parts here is the diagnostic, which already knows about the preprocessors and the datasets. Although the datasets and preprocessors have a lot of presence in the recipe, they mainly serve to make the diagnostic definition more consise.\n",
    "\n",
    "Within the Python API, there is much more flexibility to define these, and the preprocessors / datasets are directly attached to the diagnostic.\n",
    "\n",
    "To put together a recipe, we must initialize a new recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe('climwip')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe = Recipe('climwip')\n",
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add the diagnostic(s) to the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.add_diagnostic('calculate_weights_climwip', calculate_weights_climwip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Before saving the recipe, we should add the authors using the specified methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.set_description(\"EUCP ClimWIP\")\n",
    "recipe.set_authors(\n",
    "    'kalverla_peter',\n",
    "    'smeets_stef',\n",
    "    'brunner_lukas',\n",
    "    'camphuijsen_jaro',\n",
    ")\n",
    "recipe.set_maintainer(\n",
    "    'kalverla_peter',\n",
    "    'smeets_stef',\n",
    "    'brunner_lukas',\n",
    ")\n",
    "recipe.set_references(\n",
    "    'brunner2019',\n",
    "    'lorenz2018',\n",
    "    'knutti2017',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the recipe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe saved to 'climwip_from_notebook.yml'\n"
     ]
    }
   ],
   "source": [
    "recipe.save('climwip_from_notebook.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session directory: jupyter_recipe-20201106_153845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015090811d9a4780826017f5ffdb2bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Diagnostic('weighting/climwip.py')\n",
      "\n",
      "Working on 'dataset_tas'\n",
      "\n",
      "Working on 'dataset_pr'\n",
      "\n",
      "Running script 'weighting/climwip.py'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipe.run(session='jupyter_recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.show() # -> should print the recipe, but it does not work yet ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
